---
title: "comp2bdt: An R package for comparing binary diagnostic tests on paired data"
author: "Kyle Wilson"
format: pdf
---

# Introduction

Binary diagnostic tests are among the most commonly used tests in medicine and are used to rule in or out a certain condition. Commonly, this condition is disease status, but tests may also detect, for example, the presence of a bacteria or a virus, independent of any clinical manifestations. 

Test metrics such as diagnostic accuracies, predictive values and likelihood ratios are useful tools to evaluate the efficacy of such tests in comparison to a gold standard, however, these statistics only provide a description of the quality of a test. Performing statistical inference to evaluate if one test is better than another while simultaneously referencing the gold standard is more complicated. Several authors have invested significant effort into developing statistical methods to perform such inference.

Understanding and implementing methods described in the statistical literature is often far outside the comfort zone of clinicians, particularly those who are not routinely involved in academic research. Statistical complexity can therefore be a barrier to clinicians accessing and using up-to-date statistical methods. Recently, the COVID-19 pandemic has highlighted the need to be able to rapidly develop and evaluate diagnostic tests. 

Here we describe the development of a new R package which performs both descriptive and inferential statistics on paired data for two binary diagnostic tests in a single function call.

# Aims

To produce a package for the R open-source programming language which rapidly and effectively compares two binary diagnostic tests and reports descriptive and inferential statistics. Specifically, the package should:

* take a data frame or matrix as an argument containing all commonly used binary operators (eg. yes/no, y/n, pos/neg, 1/0, etc.) 
* return output following a single function call 
* display a contingency table (confusion matrix) summarising the raw data 
* allow the user to select whether this matrix has margins displaying row and column sums
* provide the prevalence of the condition in question and a confidence interval from gold standard results 
* allow the user to select which pairs of test metrics they are interested in (eg. sensitivity/specificity)  
* return a matrix for each selected test metric displaying point estimates for both tests, alongside standard errors and confidence intervals 
* return test statistics and p-values for difference between the selected test metrics between the two tests 
* handle multiple comparisons 
* offer the user the option of continuity correction if McNemar's test is indicated 
* allow the user to input test names to facilitate interpretation 
* provide an optional function which interprets the output (in plain English) for the user

# Methods

## Code development
We developed an R package by extending publicly available open-source code, which served as the initial foundation for our package. The statistical reasoning has already been well-described by Roldan-Nofuentes. The code was published as supplementary material to the previously cited paper and was released under a GPL-2 license. We enhanced and modified the code to incorporate additional functionalities and improvements based on our requirements. The development process involved implementing new algorithms, refining existing functions, and ensuring compatibility with the latest versions of R.

## Package testing
To ensure the reliability and accuracy of our developed package, we conducted thorough testing. This included unit tests to verify the correctness of individual functions and integration tests to assess the overall behavior of the package. We employed the R package testing framework, including the testthat package, to automate and streamline the testing process. Various test cases were designed to cover a wide range of scenarios and edge cases, ensuring the robustness of the package. 

## Comparative analysis
We compared our package with an existing package, DTComPair, which provides similar functionality. To perform the comparison, we utilized a publicly available dataset that has previously been used for benchmarking in the field. We evaluated both packages on multiple metrics, such as usability, performance, and computational efficiency, to identify potential strengths and weaknesses in each implementation. **this will be the next step going forward**

# Package development

## Recoding data to an acceptable format

We first defined what would be considered acceptable input to the comp2bdt function. 

Data should:

* be provided as a data frame or matrix with two or three columns 
* use common coding systems for binary data (eg. yes/no, pos/neg) 
* be free of NAs 
* be free of factor variables

The `check.df()` function ensures data meets these basic requirements.

```{r}
check.df <- function(df) {

  # convert matrices to data frame

  if(is.matrix(df)) {
    df <- as.data.frame(df)
  }

  if(!is.data.frame(df)) stop("Data should be provided as a data frame or matrix.")
  if(ncol(df) < 2 | ncol(df) > 3) stop("Data should be provided as a data frame or matrix with two or three columns.")

  # define permissible values

  allowed_char <- c(
    "positive", "pos", "p", "yes", "y", "+", "1",
    "negative", "neg", "no", "n", "-", "0", "2"
  )

  allowed_num <- c(
    0, 1, 2
  )

  # convert to lowercase for case-insensitive matching

  df <- as.data.frame(lapply(df, function(col) {
    if(is.character(col)) {
      tolower(col)
    } else {
      col
    }
  }))

  # re-code NA synonyms

  df[df == "na" | df == "not applicable" | df == "n/a"] <- NA

  all_allowed <- all(sapply(df, function(col) all((col %in% allowed_char | col %in% allowed_num) & !is.na(col))))

  if(any(sapply(df, is.factor))) stop("Values in data frame not permissable. Factors are not supported. See ?check.df for more details.")
  if(any(sapply(df, is.na))) stop("Values in data frame not permissable. NAs are not supported. See ?check.df for more details.")
  if(all_allowed == FALSE) stop("Values in data frame not permissable. Coding errors exist. See ?check.df for more details.")

}
```

Subsequently, data is re-coded (to 1/0 for yes/no, respectively) and structured as a data frame containing only integer values with the `recoder()` function.

```{r}
recoder <- function(df) {

  check.df(df)

  pos <- c("positive", "pos", "p", "yes", "y", "+", "1")
  pos.recode <- 1

  neg <- c("negative", "neg", "no", "n", "-", "0", "2")
  neg.recode <- 0

  df <- as.data.frame(lapply(df, function(col) {
    col <- as.character(col)
    col <- tolower(trimws(col))
    col[col %in% tolower(pos)] <- pos.recode
    col[col %in% tolower(neg)] <- neg.recode
    as.integer(col)
  }))

  return(df)

}
```

## Create test data 

This was used during development and can be used here, but this will probably be moved later to 'Motivating example'
```{r}
library(DTComPair) # package with similar function - was defunct when I started, has now been reincarnated - does not handle data in single function call or multiple comparisons
dat <- Paired1
dat <- data.frame(dat$y1, dat$y2, dat$d)
dat <- as.data.frame(sapply(dat, as.integer))
```

## Establishing values from binary data

Next, we need to establish values for future calculations from the user-inputted data. `s` denotes positive status (ie. gold standard is positive) while `r` indicates negative status. For each test, `1` indicates positive status and `0` indicates negative status such that `s01` indicates a patient with positive status got a negative result in Test 1 and and positive result in Test 2. Similarly, `r1` would indicate that only one test was performed and that a patient with negative status got a positive test result. 

Using these numbers it is possible to calculate disease prevalence and test metrics, as well as probabilities and variance for one or two tests (and a gold standard).

These functions are utilised internally after `check.df()` and `recoder()` so additional checks of the arguments are only required to ensure the data frame has appropriate numbers of columns. Otherwise, the output is already acceptable. 

```{r}
values.1test <- function(df) {

  ## CHECK ARGUMENTS

  if(ncol(df) != 2) stop("Please provide data as a data frame with two columns.")

  ## CALCULATE VALUES

  s1 <- sum(df[,2] == 1 & df[,1] == 1)
  s0 <- sum(df[,2] == 1 & df[,1] == 0)
  r1 <- sum(df[,2] == 0 & df[,1] == 1)
  r0 <- sum(df[,2] == 0 & df[,1] == 0)

  ss <- s1 + s0
  rr <- r1 + r0
  n1 <- s1 + r1
  n0 <- s0 + r0
  n <- s1 + s0 + r1 + r0

  p1 <- s1/n
  p0 <- s0/n
  q1 <- r1/n
  q0 <- r0/n

  prev <- ss/n
  qrev <- 1 - (ss/n)

  Se1 <- s1 / (s1 + s0)
  Sp1 <- r0 / (r0 + r1)

  PPV1 <- (prev*Se1) / (prev*Se1 + qrev*(1-Sp1))
  NPV1 <- (qrev*Sp1) / (prev*(1-Se1) + qrev*Sp1)

  PLR1 <- Se1 / (1 - Sp1)
  NLR1 <- (1 - Se1) / Sp1

  VarSe1 <- Se1 * (1 - Se1) / (n * prev)
  VarSp1 <- Sp1 * (1 - Sp1) / (n * qrev)

  VarPPV1 <- s1*r1/(n*(s1 + r1)^3)
  VarNPV1 <- s0*r0/(n*(s0 + r0)^3)

  ## VAR of PPV/NPV - ALSO SEEMS WRONG IN 2test function

  VarPLR1 <- (Se1^2*VarSp1 + (1-Sp1)^2*VarSe1)/(1-Sp1)^4
  VarNLR1 <- ((1-Se1)^2*VarSp1 + Sp1^2*VarSe1)/Sp1^4

  vals <- list(
    s1 = s1, s0 = s0, r1 = r1, r0 = r0,
    ss = ss, rr = rr, n1 = n1, n0 = n0, n = n,
    p1 = p1, p0 = p0, q1 = q1, q0 = q0,
    prev = prev, qrev = qrev,
    Se1 = Se1, Sp1 = Sp1,
    PPV1 = PPV1, NPV1 = NPV1,
    PLR1 = PLR1, NLR1 = NLR1,
    VarSe1 = VarSe1, VarSp1 = VarSp1,
    VarPPV1 = VarPPV1, VarNPV1 = VarNPV1,
    VarPLR1 = VarPLR1, VarNLR1 = VarNLR1
  )

  class(vals) <- "vals.1test"
  return(vals)

}

values.2test <- function(df) {

  ## CHECK ARGUMENTS

  if(ncol(df) != 3) stop("Please provide data as a data frame with three columns.")

  ## CALCULATE VALUES

  s11 <- sum(df[,3] == 1 & df[,1] == 1 & df[,2] == 1)
  s10 <- sum(df[,3] == 1 & df[,1] == 1 & df[,2] == 0)
  s01 <- sum(df[,3] == 1 & df[,1] == 0 & df[,2] == 1)
  s00 <- sum(df[,3] == 1 & df[,1] == 0 & df[,2] == 0)
  r11 <- sum(df[,3] == 0 & df[,1] == 1 & df[,2] == 1)
  r10 <- sum(df[,3] == 0 & df[,1] == 1 & df[,2] == 0)
  r01 <- sum(df[,3] == 0 & df[,1] == 0 & df[,2] == 1)
  r00 <- sum(df[,3] == 0 & df[,1] == 0 & df[,2] == 0)

  ss <- sum(s11, s10, s01, s00)
  rr <- sum(r11, r10, r01, r00)
  n11 <- s11 + r11
  n10 <- s10 + r10
  n01 <- s01 + r01
  n00 <- s00 + r00
  n <- sum(s11, s10, s01, s00, r11, r10, r01, r00)

  p11 <- s11/n
  p10 <- s10/n
  p01 <- s01/n
  p00 <- s00/n
  q11 <- r11/n
  q10 <- r10/n
  q01 <- r01/n
  q00 <- r00/n

  prev <- ss/n
  qrev <- 1 - (ss/n)

  Se1 <- (s11 + s10) / ss
  Se2 <- (s11 + s01) / ss
  Sp1 <- (r00 + r01) / rr
  Sp2 <- (r00 + r10) / rr

  PPV1 <- (prev * Se1) / (prev * Se1 + qrev * (1 - Sp1))
  PPV2 <- (prev * Se2) / (prev * Se2 + qrev * (1 - Sp2))
  NPV1 <- (qrev * Sp1) / (prev * (1 - Se1) + qrev * Sp1)
  NPV2 <- (qrev * Sp2) / (prev * (1 - Se2) + qrev * Sp2)

  PLR1 <- Se1 / (1 - Sp1)
  PLR2 <- Se2 / (1 - Sp2)
  NLR1 <- (1 - Se1) / Sp1
  NLR2 <- (1 - Se2) / Sp2

  VarSe1 <- Se1 * (1- Se1) / (n * prev)
  VarSe2 <- Se2 * (1- Se2) / (n * prev)
  VarSp1 <- Sp1 * (1- Sp1) / (n * qrev)
  VarSp2 <- Sp2 * (1- Sp2) / (n * qrev)

  VarPPV1 <- (s10 + s11)*(r10 + r11)/(n*(s10 + s11 + r10 + r11)^3)
  VarPPV2 <- (s01 + s11)*(r01 + r11)/(n*(s01 + s11 + r01 + r11)^3)

  VarNPV1 <- (s00 + s01)*(r00 + r01)/(n*(s00 + s01 + r00 + r01)^3)
  VarNPV2 <- (s00 + s10)*(r00 + r10)/(n*(s00 + s10 + r00 + r10)^3)

  VarPLR1 <- (Se1^2*VarSp1 + (1-Sp1)^2*VarSe1)/(1-Sp1)^4
  VarPLR2 <- (Se2^2*VarSp2 + (1-Sp2)^2*VarSe2)/(1-Sp2)^4

  VarNLR1 <- ((1-Se1)^2*VarSp1 + Sp1^2*VarSe1)/Sp1^4
  VarNLR2 <- ((1-Se2)^2*VarSp2 + Sp2^2*VarSe2)/Sp2^4

  vals <- list(s11 = s11, s10 = s10, s01 = s01, s00 = s00,
               r11 = r11, r10 = r10, r01 = r01, r00 = r00,
               ss = ss, rr = rr,
               n11 = n11, n10 = n10, n01 = n01, n00 = n00, n = n,
               p11 = p11, p10 = p10, p01 = p01, p00 = p00,
               q11 = q11, q10 = q10, q01 = q01, q00 = q00,
               prev = prev, qrev = qrev,
               Se1 = Se1, Se2 = Se2, Sp1 = Sp1, Sp2 = Sp2,
               PPV1 = PPV1, PPV2 = PPV2, NPV1 = NPV1, NPV2 = NPV2,
               PLR1 = PLR1, PLR2 = PLR2, NLR1 = NLR1, NLR2 = NLR2,
               VarSe1 = VarSe1, VarSe2 = VarSe2,
               VarSp1 = VarSp1, VarSp2 = VarSp2,
               VarPPV1 = VarPPV1, VarPPV2 = VarPPV2,
               VarNPV1 = VarNPV1, VarNPV2 = VarNPV2,
               VarPLR1 = VarPLR1, VarPLR2 = VarPLR2,
               VarNLR1 = VarNLR1, VarNLR2 = VarNLR2)

  class(vals) <- "vals.2test"
  return(vals)

}
```

In published literature describing test metrics data are very often presented as a contingency table (or confusion matrix) showing the frequency of each combination of events. We provide this output to the user via the `disp.cont()` function. 

```{r}
disp.cont <- function(vals, margins = FALSE) {

  ## CHECK ARGUMENTS

  if(!(class(vals) %in% c("vals.1test", "vals.2test"))){
    stop("Argument must be of \"vals.1test\" or \"vals.2test\" class.")
  }

  ## COMPUTE CONTINGENCY TABLE

  if(class(vals) == "vals.1test") {

    mat <- matrix(c(vals$s1, vals$r1, vals$s0, vals$r0), nrow = 2)

    dimnames(mat) <- list("True Status" = c("Positive","Negative"),
                          "Test" = c("Positive","Negative"))

    if(margins == TRUE) {
      mat <- addmargins(mat)
    }

    return(mat)

  } else if(class(vals) == "vals.2test") {

    mat1 <- matrix(c(vals$s11, vals$s01, vals$s10, vals$s00), nrow = 2)
    dimnames(mat1) <- list("Test 1" = c("Positive","Negative"),
                           "Test 2" = c("Positive","Negative"))

    mat2 <- matrix(c(vals$r11, vals$r01, vals$r10, vals$r00), nrow = 2)
    dimnames(mat2) <- list("Test 1" = c("Positive","Negative"),
                           "Test 2" = c("Positive","Negative"))

    if(margins == TRUE) {
      mat1 <- addmargins(mat1)
      mat2 <- addmargins(mat2)
    }

    mat <- list(mat1, mat2)
    names(mat) <- c("True Status: POS", "True Status: NEG")

    return(mat)

  }

}
```

## Defining confidence intervals

The next thing that we need to do is define confidence intervals. We establish separate functions that provide confidence intervals for each of the point estimates, as they are not always calculated in the same way. 

Several of the confidence intervals are constructed using the interval for binomial proportions described by Yu et al. First we define a function to calculate the Yu et al. confidence interval, then we apply it where necessary in our tests. It should be noted that when total number of cases is very small and the point estimate is 1 the Yu interval can return values which are very slightly greater than 1. As greater than 100% confidence is meaningless and the overshoot in very tiny this has been compensated, such that it doesn't affect results when a user wants a high level of precision (many decimal places).

```{r}

## DEFINING YU ET AL INTERVAL

yu.int <- function(n, z, st) {
  lower <- 0.5 + ((n + z^4 / 53) / (n + z^2)) * (st - 0.5) - (z / (n + z^2)) * sqrt(n * st * (1 - st) + z^2 / 4)
  upper <- 0.5 + ((n + z^4 / 53) / (n + z^2)) * (st - 0.5) + (z / (n + z^2)) * sqrt(n * st * (1 - st) + z^2 / 4)
  if(upper <= 1) {return(list(lower = lower, upper = upper))} else {return(list(lower = lower, upper = 1))}
}

conf.prev <- function(vals, alpha = 0.05, dp = 3) {
  
  ## CHECK ARGUMENTS
  
    if(!(class(vals) %in% c("vals.1test", "vals.2test"))){
    stop("Argument must be of \"vals.1test\" or \"vals.2test\" class.")
    }
  
  ## CALCULATE CONFIDENCE INTERVALS
    
    conf <- 1 - alpha
    z <- qnorm(1 - alpha / 2)
    
    yu <- yu.int(vals$n, z, vals$prev)
    
    ci.prev <- list(
      est = vals$prev,
      ci.lower = yu$lower, 
      ci.upper = yu$upper
      )
    
    class(ci.prev) <- "prev"
    return(ci.prev)
    
} 
  

conf.acc <- function(vals, alpha = 0.05, dp = 3, test.names = c("Test 1", "Test 2")) {
  
  ## CHECK ARGUMENTS
  
    if(!(class(vals) %in% c("vals.1test", "vals.2test"))){
    stop("Argument must be of \"vals.1test\" or \"vals.2test\" class.")
    }
  
  ## CALCULATE CONFIDENCE INTERVALS
  
  conf <- 1 - alpha
  z <- qnorm(1 - alpha / 2)
  
  if(class(vals) == "vals.2test"){
    
    yu_Se1 <- yu.int(vals$n, z, vals$Se1)
    yu_Se2 <- yu.int(vals$n, z, vals$Se2)
    yu_Sp1 <- yu.int(vals$n, z, vals$Sp1)
    yu_Sp2 <- yu.int(vals$n, z, vals$Sp2)

    ci.acc <- list(
      test1.se = list(
        est = vals$Se1,
        SE = sqrt(vals$VarSe1),
        ci.lower = yu_Se1$lower, 
        ci.upper = yu_Se1$upper), 
      test2.se = list(
        est = vals$Se2,
        SE = sqrt(vals$VarSe2),
        ci.lower = yu_Se2$lower, 
        ci.upper = yu_Se2$upper),
      test1.sp = list(
        est = vals$Sp1,
        SE = sqrt(vals$VarSp1),
        ci.lower = yu_Sp1$lower, 
        ci.upper = yu_Sp1$upper), 
      test2.sp = list(
        est = vals$Sp2,
        SE = sqrt(vals$VarSp2),
        ci.lower = yu_Sp2$lower, 
        ci.upper = yu_Sp2$upper)
      )
    
    class(ci.acc) <- "conf.2t"
    
    return(ci.acc)
    
  } else if(class(vals) == "vals.1test"){
    
    yu_Se1 <- yu.int(vals$n, z, vals$Se1)
    yu_Sp1 <- yu.int(vals$n, z, vals$Sp1)
    
    ci.acc <- list(
      se = list(
        est = vals$Se1,
        SE = sqrt(vals$VarSe1),
        ci.lower = yu_Se1$lower, 
        ci.upper = yu_Se1$upper),
      sp = list(
        est = vals$Sp1,
        SE = sqrt(vals$VarSp1),
        ci.lower = yu_Sp1$lower, 
        ci.upper = yu_Sp1$upper)
      )
    
    class(ci.acc) <- "conf.1t"
    
    return(ci.acc)
    
  } 
  
}

conf.pv <- function(vals, alpha = 0.05, dp = 3, test.names = c("Test 1", "Test 2")) {
  
  ## CHECK ARGUMENTS
  
    if(!(class(vals) %in% c("vals.1test", "vals.2test"))){
    stop("Argument must be of \"vals.1test\" or \"vals.2test\" class.")
    }
  
  ## CALCULATE CONFIDENCE INTERVALS
  
  conf <- 1 - alpha
  z <- qnorm(1 - alpha / 2)
  
  if(class(vals) == "vals.2test"){
    
    yu_PPV1 <- yu.int(vals$n11 + vals$n01, z, vals$PPV1)
    yu_PPV2 <- yu.int(vals$n10 + vals$n01, z, vals$PPV2)
    yu_NPV1 <- yu.int(vals$n01 + vals$n00, z, vals$NPV1)
    yu_NPV2 <- yu.int(vals$n10 + vals$n00, z, vals$NPV2)
    
    ci.pv <- list(
      test1.ppv = list(
        est = vals$PPV1,
        SE = sqrt(vals$VarPPV1),
        ci.lower = yu_PPV1$lower, 
        ci.upper = yu_PPV1$upper), 
      test2.ppv = list(
        est = vals$PPV2,
        SE = sqrt(vals$VarPPV2),
        ci.lower = yu_PPV2$lower, 
        ci.upper = yu_PPV2$upper),
      test1.npv = list(
        est = vals$NPV1,
        SE = sqrt(vals$VarPPV2),
        ci.lower = yu_NPV1$lower, 
        ci.upper = yu_NPV1$upper), 
      test2.npv = list(
        est = vals$NPV2,
        SE = sqrt(vals$VarNPV2),
        ci.lower = yu_NPV2$lower, 
        ci.upper = yu_NPV2$upper)
      )
    
    class(ci.pv) <- "conf.2t"
    
    return(ci.pv)
    
  } else if(class(vals) == "vals.1test"){
    
    yu_PPV1 <- yu.int(vals$n, z, vals$PPV1)
    yu_NPV1 <- yu.int(vals$n, z, vals$NPV1)
    
    ci.pv <- list(
      PPV = list(
        est = vals$PPV1,
        SE = sqrt(vals$VarPPV1),
        ci.lower = yu_PPV1$lower, 
        ci.upper = yu_PPV1$upper),
      NPV = list(
        est = vals$NPV1,
        SE = sqrt(vals$VarNPV1),
        ci.lower = yu_NPV1$lower, 
        ci.upper = yu_NPV1$upper)
      )
    
    class(ci.pv) <- "conf.1t"
    
    return(ci.pv)
    
  }
    
}

```

The confidence intervals for the likelihood ratios cannot be constructed from the Yu et al. interval. They are constructed based on the best method for defining confidence intervals for ratios of independent binomial proportions as described by Martín-Andrés and Álvarez-Hernández.

```{r}

ciplr <- function(s1, s0, r1, r0, z) {
  
  ss1 = s1 + s0
  rr1 = r1 + r0
  nn1 = ss1 + rr1
    
  p1 = (r1 + 0.5) / (rr1 + 1)
  p2 = (s1 + 0.5) / (ss1 + 1)
    
  Lplr <- ((nn1 + 2) * (s1 + 0.5) * (r1 + 0.5) + (z^2 / 2) * ((ss1 + 1) * (s1 + 0.5) + (rr1 + 1) * (r1 + 0.5) - 2 * (s1 + 0.5) * (r1 + 0.5)) - z * sqrt(((nn1 + 2)^2 * (r1 + 0.5) * (s1 + 0.5) * ((s1 + r1 + 1) - (nn1 + 2) * p1 * p2) + (z^2 / 4) * ((ss1 + 1) * (s1 + 0.5) - (rr1 + 1) * (r1 + 0.5))^2))) /  ((r1 + 0.5) * ((nn1 + 2) * (ss1 + 1) * p1 - z^2 * ((ss1 + 1) - (r1 + 0.5))))
  
  Uplr <- ((nn1 + 2) * (r1 + 0.5) * (s1 + 0.5) + (z^2 / 2) * ((ss1 + 1) * (s1 + 0.5) + (rr1 + 1) * (r1 + 0.5) - 2 * (r1 + 0.5) * (s1 + 0.5)) + z * sqrt(((nn1 + 2)^2 * (r1 + 0.5) * (s1 + 0.5) * ((s1 + r1 + 1) - (nn1 + 2) * p1 * p2) + (z^2 / 4) * ((ss1 + 1) * (s1 + 0.5) - (rr1 + 1) * (r1 + 0.5))^2))) /  ((r1 + 0.5) * ((nn1 + 2) * (ss1 + 1) * p1 - z^2 * ((ss1 + 1) - (r1 + 0.5))))
    
  if (Lplr < (s1 + 0.5) / ((nn1 + 2) - (r1 + 0.5))) {
    Lplr <- ((s1 + 0.5) * p1 + z^2 / 2 - z * sqrt(z^2 / 4 + (s1 + 0.5) * (p1 - p2))) / ((ss1 + 1) * p1^2 + z^2)
  }
  if (Uplr > ((nn1 + 2) - (s1 + 0.5)) / (r1 + 0.5)) {
    Uplr <- ((r1 + 0.5) * p2 + z^2 / 2 + z * sqrt(z^2 / 4 + (r1 + 0.5) * (p2 - p1))) / ((rr1 + 1) * p1^2)
  }
  
  ciPLR <- list(LPLR = Lplr, UPLR = Uplr)
  
}

cinlr = function(s1, s0, r1, r0, z) {
    
  ss1 = s1 + s0
  rr1 = r1 + r0
  nn1 = ss1 + rr1
    
  p1 = (r0 + 0.5) / (rr1 + 1) 
  p2 = (s0 + 0.5) / (ss1 + 1)
    
  Lnlr <- ((nn1 + 2) * (r0 + 0.5) * (s0 + 0.5) + (z^2 / 2) * ((ss1 + 1) * (s0 + 0.5) + (rr1 + 1) * (r0 + 0.5) - 2 * (r0 + 0.5) * (s0 + 0.5)) - z * sqrt(((nn1 + 2)^2 * (r0 + 0.5) * (s0 + 0.5) * ((s0 + r0 + 1) - (nn1 + 2) * p1 * p2) + (z^2 / 4) * ((ss1 + 1) * (s0 + 0.5) - (rr1 + 1) * (r0 + 0.5))^2))) / ((r0 + 0.5) * ((nn1 + 2) * (ss1 + 1) * p1 - z^2 * ((ss1 + 1) - (r0 + 0.5))))
    
  Unlr <- ((nn1 + 2) * (r0 + 0.5) * (s0 + 0.5) + (z^2 / 2) * ((ss1 + 1) * (s0 + 0.5) + (rr1 + 1) * (r0 + 0.5) - 2 * (r0 + 0.5) * (s0 + 0.5)) + z * sqrt(((nn1 + 2)^2 * (r0 + 0.5) * (s0 + 0.5) * ((s0 + r0 + 1) - (nn1 + 2) * p1 * p2) + (z^2 / 4) * ((ss1 + 1) * (s0 + 0.5) - (rr1 + 1) * (r0 + 0.5))^2))) / ((r0 + 0.5) * ((nn1 + 2) * (ss1 + 1) * p1 - z^2 * ((ss1 + 1) - (r0 + 0.5))))
    
  if (Lnlr < (s0 + 0.5) / ((nn1 + 2) - (r0 + 0.5))) {
    Lnlr <- ((s0 + 0.5) * p1 + z^2 / 2 - z * sqrt(z^2 / 4 + (s0 + 0.5) * (p1 - p2))) / ((ss1 + 1) * p1^2 + z^2)
  }
  if (Unlr > ((nn1 + 2) - (s0 + 0.5)) / (r0 + 0.5)) {
    Unlr <- ((r0 + 0.5) * p2 + z^2 / 2 + z * sqrt(z^2 / 4 + (r0 + 0.5) * (p2 - p1))) / ((rr1 + 1) * p1^2)
  }
  
  ciNLR <- list(LNLR = Lnlr, UNLR = Unlr)
  
}

conf.lr <- function(vals, alpha = 0.05, dp = 3, test.names = c("Test 1", "Test 2")) {
  
  ## CHECK ARGUMENTS
  
    if(!(class(vals) %in% c("vals.1test", "vals.2test"))){
    stop("Argument must be of \"vals.1test\" or \"vals.2test\" class.")
    }
  
  ## CALCULATE CONFIDENCE INTERVALS
  
  conf <- 1 - alpha
  z <- qnorm(1 - alpha / 2)
  
  if(class(vals) == "vals.2test"){
    
    ciPLR1 <- ciplr(vals$s11 + vals$s10, vals$s01+ vals$s00, 
                    vals$r11 + vals$r10, vals$r01 + vals$r00,z)
    ciPLR2 <- ciplr(vals$s11+ vals$s01, vals$s10+ vals$s00, 
                    vals$r11 + vals$r01, vals$r10 + vals$r00,z)
    
    ciNLR1 <- cinlr(vals$s11+ vals$s10, vals$s01+ vals$s00, 
                    vals$r11 + vals$r10, vals$r01 + vals$r00,z)
    ciNLR2 <- cinlr(vals$s11+ vals$s01, vals$s10+ vals$s00, 
                    vals$r11 + vals$r01, vals$r10 + vals$r00,z)
    
    ci.lr <- list(
      test1.plr = list(
        est = vals$PLR1,
        SE = sqrt(vals$VarPLR1),
        ci.lower = ciPLR1$LPLR, 
        ci.upper = ciPLR1$UPLR), 
      test2.plr = list(
        est = vals$PLR2,
        SE = sqrt(vals$VarPLR2),
        ci.lower = ciPLR2$LPLR, 
        ci.upper = ciPLR2$UPLR),
      test1.nlr = list(
        est = vals$NLR1,
        SE = sqrt(vals$VarNLR1),
        ci.lower = ciNLR1$LNLR, 
        ci.upper = ciNLR1$UNLR), 
      test2.nlr = list(
        est = vals$NLR2,
        SE = sqrt(vals$VarNLR2),
        ci.lower = ciNLR2$LNLR, 
        ci.upper = ciNLR2$UNLR)
    )
    
    class(ci.lr) <- "conf.2t"
    
    return(ci.lr)
    
  } else if(class(vals) == "vals.1test") {
    
    ciPLR1 <- ciplr(vals$s1, vals$s0, 
                    vals$r1, vals$r0, z)
    
    ciNLR1 <- cinlr(vals$s1, vals$s0, 
                    vals$r1, vals$r0, z)
    
    ci.lr <- list(
      test1.plr = list(
        est = vals$PLR1,
        SE = sqrt(vals$VarPLR1),
        ci.lower = ciPLR1$LPLR, 
        ci.upper = ciPLR1$UPLR), 
      test1.nlr = list(
        est = vals$NLR1,
        SE = sqrt(vals$VarNLR1),
        ci.lower = ciNLR1$LNLR, 
        ci.upper = ciNLR1$UNLR)
    )
    
    class(ci.lr) <- "conf.1t"
    
    return(ci.lr)
    
  }
  
}
```

## Hypothesis testing

Several authors have recommended global hypothesis testing for test metrics. For example, regarding diagnostic accuracies (sensitivity and specificity), it is preferable to  test simultaneously according to the null hypothesis H0: sensitivity Test 1 = sensitivity Test 2 & specificity Test 1 = specificity Test 2, with further individual hypothesis testing (H0: sensitivity Test 1 = sensitivity Test 2; H0: specificity Test 1 = specificity Test 2) to identify the cause of any significant difference in the global hypothesis. This approach is applied also to predictive values and likelihood ratios. 

Interestingly, a simulation study suggested that when comparing sensitivity and specificity it is preferable to use McNemars test for individual hypotheses when disease prevalence is low (< 0.1) and n <= 100. 

We defined three functions which perform hypothesis tests for diagnostic accuracies, predictive values and likelihood ratios.

```{r}

output.acc <- function(vals) {
  
  ## CHECK ARGUMENTS
  
    if(class(vals) != "vals.2test"){
    stop("Argument must be of \"vals.2test\" class.")
    }
  
  ## GLOBAL HYPOTHESIS TEST
  
  Q1 <- vals$ss * (vals$s10 - vals$s01)^2 / (4 * vals$s10* vals$s01+ (vals$s11+ vals$s00) * (vals$s10+ vals$s01)) + vals$rr * (vals$r10 - vals$r01)^2 / (4 * vals$r10 * vals$r01 + (vals$r11 + vals$r00) * (vals$r10 + vals$r01))
  
  globalpvalue1 <- (1 - pchisq(Q1, 2))
  
  ## INDIVIDUAL HYPOTHESIS TESTS
  ## WALD TEST STATISTIC AND P-VALUES
  
  w1 <- (vals$ss * (vals$s10- vals$s01)^2) / (4 * vals$s10* vals$s01+ (vals$s11+ vals$s00) * (vals$s10+ vals$s01))       
  w2 <- (vals$rr * (vals$r10 - vals$r01)^2) / (4 * vals$r10 * vals$r01 + (vals$r11 + vals$r00) * (vals$r10 + vals$r01))
  
  pvalue1 <- 2 * (1 - pnorm(w1, 0, 1))
  pvalue2 <- 2 * (1 - pnorm(w2, 0, 1))
  
  ## MCNEMARS CHI-SQUARED AND P-VALUES
  
  Mcc1 <- (abs(vals$s10- vals$s01) - 1)^2 / (vals$s10+ vals$s01)
  Mcc2 <- (abs(vals$r10 - vals$r01) - 1)^2 / (vals$r10 + vals$r01)
  
  pvalue3a <- 2 * (1 - pnorm(Mcc1, 0, 1))
  pvalue4a <- 2 * (1 - pnorm(Mcc2, 0, 1))
  
  M1 <- abs(vals$s10- vals$s01)^2 / (vals$s10+ vals$s01)
  M2 <- abs(vals$r10 - vals$r01)^2 / (vals$r10 + vals$r01)
  
  pvalue3b <- 2 * (1 - pnorm(M1, 0, 1))
  pvalue4b <- 2 * (1 - pnorm(M2, 0, 1))
  
  return(list(glob.t = Q1, glob.p = globalpvalue1, 
              ind.t1 = w1, ind.t2 = w2, 
              ind.p1 = pvalue1, ind.p2 = pvalue2,
              Mcc1 = Mcc1, Mcc2 = Mcc2,
              pval3a = pvalue3a, pval4a = pvalue4a,
              M1 = M1, M2 = M2,
              pval3b = pvalue3b, pval4b = pvalue4b))

}

output.pv <- function(vals) {
  
  ## CHECK ARGUMENTS
  
    if(class(vals) != "vals.2test"){
    stop("Argument must be of \"vals.2test\" class.")
    }
  
  ## ESTABLISH MATRICES
  
  Var <- matrix(0,4,4) #Variances - covariances matrix
  
  Var[1,1] <- ((vals$p10 + vals$p11) * (vals$q10 + vals$q11)) / (vals$n * (vals$p10 + vals$p11 + vals$q10 + vals$q11)^3)
  
  Var[1,2] <- (vals$p01 * vals$p10 * vals$q11 + vals$p11 * (vals$q01 * (vals$q10 + vals$q11) + vals$q11 * (vals$p01 + vals$p10 + vals$p11 + vals$q10 + vals$q11))) / (vals$n * (vals$p01 + vals$p11 + vals$q01 + vals$q11)^2 * (vals$p10 + vals$p11 + vals$q10 + vals$q11)^2)
  
  Var[1,3] <- 0 
  
  Var[1,4] <- -(vals$p00 * (vals$p10 + vals$p11) * vals$q10 + vals$p10 * vals$q10 * (vals$p10 + vals$p11 + vals$q00 + vals$q10) + vals$p10 * (vals$q00 + vals$q10) * vals$q11) / (vals$n * (vals$p00 + vals$p10 + vals$q00 + vals$q10)^2 * (vals$p10 + vals$p11 + vals$q10 + vals$q11)^2)
  
  Var[2,1] <- (vals$p01 * vals$p10 * vals$q11 + vals$p11 * (vals$q01 * (vals$q10 + vals$q11) + vals$q11 * (vals$p01 + vals$p10 + vals$p11 + vals$q10 + vals$q11))) / (vals$n * (vals$p01 + vals$p11 + vals$q01 + vals$q11)^2 * (vals$p10 + vals$p11 + vals$q10 + vals$q11)^2)
  
  Var[2,2] <- ((vals$p01 + vals$p11) * (vals$q01 + vals$q11)) / (vals$n * (vals$p01 + vals$p11 + vals$q01 + vals$q11)^3)
  
  Var[2,3] <- -(vals$p00 * (vals$p01 + vals$p11) * vals$q01 + vals$p01 * vals$q01 * (vals$p01 + vals$p11 + vals$q00 + vals$q01) + vals$p01 * (vals$q00 + vals$q01) * vals$q11) / (vals$n * (vals$p00 + vals$p01 + vals$q00 + vals$q01)^2 * (vals$p01 + vals$p11 + vals$q01 + vals$q11)^2)
  
  Var[2,4] <- 0
  
  Var[3,1] <- 0
  
  Var[3,2] <- Var[2,3]
  
  Var[3,3] <- ((vals$p00 + vals$p01) * (vals$q00 + vals$q01)) / (vals$n * (vals$p00 + vals$p01 + vals$q00 + vals$q01)^3)
  
  Var[3,4] <- (vals$q00 * (vals$p00^2 + vals$p01 * vals$p10 + vals$p00 * (vals$p01 + vals$p10 + vals$q00 + vals$q01)) + vals$p00 * (vals$q00 + vals$q01) * vals$q10) / (vals$n * (vals$p00 + vals$p01 + vals$q00 + vals$q01)^2 * (vals$p00 + vals$p10 + vals$q00 + vals$q10)^2)
  
  Var[4,1] <- Var[1,4]
  
  Var[4,2] <- 0
  
  Var[4,3] <- Var[3,4]
  
  Var[4,4] <- ((vals$p00 + vals$p10) * (vals$q00 + vals$q10)) / (vals$n * (vals$p00 + vals$p10 + vals$q00 + vals$q10)^3)
  
  phi <- matrix(0,2,4)
  
  phi[1,1] <- 1
  phi[1,2] <- -1
  
  phi[2,3] <- 1
  phi[2,4] <- -1 
  
  PV <- as.matrix(c(vals$PPV1, vals$PPV2, vals$NPV1, vals$NPV2))
  
  dim(PV)<- c(1,4)
  
  sigmaPV <- phi %*% Var %*% t(phi)
  
  if (is.nan(det(sigmaPV)) | det(sigmaPV == 0)) {
    Q2 <- NaN
  } else {
    Q2 <-  PV %*% t(phi) %*% solve(sigmaPV) %*% phi %*% t(PV)
  }
  
  globalpvalue2 <- (1 - pchisq(Q2, 2)) 
  
  PPVp <- (2 * vals$s11 + vals$s10 + vals$s01) / (2 * vals$n11 + vals$n10 + vals$n01)
  
  NPVp <- (2 * vals$r00 + vals$r01 + vals$r10) / (2 * vals$n00 + vals$n01 + vals$n10)
  
  CPPVp <- (vals$s11 * (1 - PPVp)^2 + vals$r11 * PPVp^2) / (2 * vals$n11 + vals$n10 + vals$n01)
  
  CNPVp <- (vals$s00 * NPVp^2 + vals$r00 * (1 - NPVp)^2) / (2 * vals$n00 + vals$n01 + vals$n10)
  
  T1 <- (vals$PPV1 - vals$PPV2)^2 / ((PPVp * (1 - PPVp) - 2 * CPPVp) * ((1 / (vals$n10 + vals$n11)) + (1 / (vals$n01 + vals$n11))))
  T2 <- (vals$NPV1 - vals$NPV2)^2 / ((NPVp * (1 - NPVp) - 2 * CNPVp) * ((1 / (vals$n01 + vals$n00)) + (1 / (vals$n10 + vals$n00))))
  
  pvalue5 <- (1 - pchisq(T1, 1))
  pvalue6 <- (1 - pchisq(T2, 1))
  
  return(list(glob.t = Q2, glob.p = globalpvalue2, 
              ind.t1 = T1, ind.t2 = T2, 
              ind.p1 = pvalue5, ind.p2 = pvalue6
              ))
  
}

output.lr <- function(vals) {
  
  ## CHECK ARGUMENTS
  
    if(class(vals) != "vals.2test"){
    stop("Argument must be of \"vals.2test\" class.")
    }
  
  ## ESTABLISH MATRICES
  
  logposw <- log(vals$PLR1 / vals$PLR2)
  lognegw <- log(vals$NLR1 / vals$NLR2)
  logwmat <- as.matrix(c(logposw, lognegw))
  
  mat1 <- matrix(0, 2, 8) 
  
  mat1[1,1] <- 1 / (vals$p10 + vals$p11) - 1 / (vals$p01 + vals$p11)     
  mat1[1,2] <- 1 / (vals$p10 + vals$p11)   
  mat1[1,3] <- -1 / (vals$p01 + vals$p11)   
  mat1[1,4] <- 0
  
  mat1[1,5] <- 1 / (vals$q01 + vals$q11) - 1 / (vals$q10 + vals$q11)   
  mat1[1,6] <- -1 / (vals$q10 + vals$q11)   
  mat1[1,7] <- 1/(vals$q01 + vals$q11)   
  mat1[1,8] <- 0  
  
  mat1[2,1] <- 0
  mat1[2,2] <- -1 / (vals$p00 + vals$p10)
  mat1[2,3] <- 1 / (vals$p00 + vals$p01)
  mat1[2,4] <- 1 / (vals$p00 + vals$p01) - 1 / (vals$p00 + vals$p10)
  
  mat1[2,5] <- 0
  mat1[2,6] <- 1 / (vals$q00 + vals$q10)
  mat1[2,7] <- -1 / (vals$q00 + vals$q01)
  mat1[2,8] <- 1 / (vals$q00 + vals$q10) - 1 / (vals$q00 + vals$q01)
  
  vec1 <- vector("numeric", 8)
  
  vec1[1] <- vals$p11
  vec1[2] <- vals$p10
  vec1[3] <- vals$p01
  vec1[4] <- vals$p00
  
  vec1[5] <- vals$q11
  vec1[6] <- vals$q10
  vec1[7] <- vals$q01
  vec1[8] <- vals$q00
  
  mat2 <- matrix(0, 8, 8)
  
  mat2[1,1] <- vals$p11
  mat2[2,2] <- vals$p10
  mat2[3,3] <- vals$p01
  mat2[4,4] <- vals$p00
  
  mat2[5,5] <- vals$q11
  mat2[6,6] <- vals$q10
  mat2[7,7] <- vals$q01
  mat2[8,8] <- vals$q00
  
  sigma1 <- matrix(0, 8, 8)
  sigma1 <- (1 / vals$n) * (mat2 - vec1 %*% t(vec1))
  
  sigmaLR <- matrix(0, 2, 2)
  sigmaLR <- mat1 %*% sigma1 %*% t(mat1)
  
  ## GLOBAL TESTS
  
  if(is.nan(det(sigmaLR)) | det(sigmaLR) == 0) {
    Q3 <- NaN
  } else {
    Q3 <- t(logwmat) %*% solve(sigmaLR) %*% logwmat
  }
  
  globalpvalue3 <- (1 - pchisq(Q3, 2))
  
  ## INDIVIDUAL TESTS
  
  z1 <- abs(logposw) / sqrt(sigmaLR[1, 1])
  z2 <- abs(lognegw) / sqrt(sigmaLR[2, 2])
  
  pvalue7 <- 2 * (1 - pnorm(z1, 0, 1))
  pvalue8 <- 2 * (1 - pnorm(z2, 0, 1))
  
  return(list(glob.t = Q3, glob.p = globalpvalue3, 
              ind.t1 = z1, ind.t2 = z2, 
              ind.p1 = pvalue7, ind.p2 = pvalue8
              ))
  
}
```

To render the list output of the comp2bdt function more readable the output from `conf.` functions is summarised in a table using the `matrixify()` function. In addition to the output from a `conf.` function, this function accepts row names via the `rows` argument (which correspond to the test metrics) and test names via `test.names`. 

```{r}

matrixify <- function(x, rows = c("Row 1", "Row 2"), test.names = c("Test 1", "Test 2"), dp = 3) {
  
  if(class(x) == "conf.1t") {
    
    mat <- matrix(rep(0,6), nrow = 2, ncol = 3,
         dimnames = list(c(rows[1], rows[2]),
                         c("Estimate", "Lower CI", "Upper CI"))
    ) 
  
    mat[1,1] <- round(x[[1]][[1]],dp)*100
    mat[1,2] <- round(x[[1]][[2]],dp)*100
    mat[1,3] <- round(x[[1]][[3]],dp)*100
  
    mat[2,1] <- round(x[[2]][[1]],dp)*100
    mat[2,2] <- round(x[[2]][[2]],dp)*100
    mat[2,3] <- round(x[[2]][[3]],dp)*100
  
    return(mat)
    
  } else if(class(x) == "conf.2t") {
    
    mat1 <- matrix(rep(0,8), nrow = 2, ncol = 4,
         dimnames = list(c(rows[1], rows[2]),
                         c("Estimate", "SE", "Lower CI", "Upper CI"))
    ) 
  
    mat1[1,1] <- round(x[[1]][[1]],dp)*100
    mat1[1,2] <- round(x[[1]][[2]],dp)*100
    mat1[1,3] <- round(x[[1]][[3]],dp)*100
    mat1[1,4] <- round(x[[1]][[4]],dp)*100
  
    mat1[2,1] <- round(x[[3]][[1]],dp)*100
    mat1[2,2] <- round(x[[3]][[2]],dp)*100
    mat1[2,3] <- round(x[[3]][[3]],dp)*100
    mat1[2,4] <- round(x[[3]][[4]],dp)*100
    
    mat2 <- matrix(rep(0,8), nrow = 2, ncol = 4,
         dimnames = list(c(rows[1], rows[2]),
                         c("Estimate", "SE", "Lower CI", "Upper CI"))
    ) 
  
    mat2[1,1] <- round(x[[2]][[1]],dp)*100
    mat2[1,2] <- round(x[[2]][[2]],dp)*100
    mat2[1,3] <- round(x[[2]][[3]],dp)*100
    mat2[1,4] <- round(x[[2]][[4]],dp)*100
  
    mat2[2,1] <- round(x[[4]][[1]],dp)*100
    mat2[2,2] <- round(x[[4]][[2]],dp)*100
    mat2[2,3] <- round(x[[4]][[3]],dp)*100
    mat2[2,4] <- round(x[[4]][[4]],dp)*100
  
    mat <- list(mat1, mat2)
    names(mat) <- test.names
    
    return(mat)
    
  }
  
}


```

To make the package as intuitive as possible for clinicians all functionality should be available in a single function call. This single function performs both descriptive and inferential statistics on two binary diagnostic tests. 

This function accepts several arguments and calls all of the functions previously described. Subsequently, it outputs a list containing the contingency table, prevalence and the matrix of descriptive statistics and hypothesis tests for each selected pair of test metrics. 

Below is a description of arguments and outputs.

### Arguments

* df = a data frame conforming to the previously described standards for `check.df()` 
* alpha = an alpha value, defaults to 0.05 
* margins = a Boolean value, when `TRUE` adds row and column sums to contingency tables, defaults to `FALSE` 
* multi_corr = method of correcting for multiple comparison, uses `p.adjust.methods` 
* cc = a Boolean value, when `TRUE` continuity correction is applied to McNemar's test 
* sesp =  a Boolean value, when `TRUE` diagnostic accuracies are calculated and hypothesis tests performed 
* ppvnpv =  a Boolean value, when `TRUE` predictive values are calculated and hypothesis tests performed 
* plrnlr =  a Boolean value, when `TRUE` likelihood ratios are calculated and hypothesis tests performed 
* test.names = a character vector of length 2 giving the names of the tests being compared
* dp = number of decimal places for output in descriptive matrices; test statistics and p-values are not limited by this figure

### Output

Contigency tables and prevalence data is self-explanatory. 
Examples here are for diagnostic accuracies, but output is very similar for predictive values and likelihood ratios (same pattern, eg. substitute sens. for ppv. and spec. for npv.).

* accuracies = matrix summarising point estimates, standard errors and confidence intervals for sensitivity and specifiity 
* glob.test.stat = global test statistic (H0: sensitivity Test 1 = sensitivity Test 2 & specificity Test 1 = specificity Test 2) 
* glob.p.value = global p value (H0: sensitivity Test 1 = sensitivity Test 2 & specificity Test 1 = specificity Test 2) 
* glob.p.adj = adjusted global p value (all global p values are compiled into a vector and adjusted) 
* sens.test.stat = test statistic for difference in sensitivity (H0: sensitivity Test 1 = sensitivity Test 2) 
* sens.p.value = p value for difference in sensitivity (H0: sensitivity Test 1 = sensitivity Test 2) 
* sens.p.adj = adjusted p value for difference in sensitivity (all individual p values are compiled into a vector and adjusted) 
* spec.test.stat = test statistic for difference in specificity (H0: specificity Test 1 = specificity Test 2) 
* spec.p.value = p value for difference in specificity (H0: specificity Test 1 = specificity Test 2) 
* spec.p.adj = adjusted p value for difference in specificity (all individual p values are compiled into a vector and adjusted) 

It is conceivable that following adjustment a global p-value can reach significance and both individual p-values do not reach significance. **I will attempt to identify how common this is with simulation but perhaps quite common** In these circumstances I would suggest the test is not significant. Multiple testing adjustments are independent for global and individual hypothesis tests so I suggest that it is multiple comparison which makes it not significant.

```{r}
comp2bdt <- function(df, alpha = 0.05, margins = FALSE, multi_corr = "holm", cc = TRUE,
                     sesp = TRUE, ppvnpv = TRUE, plrnlr = TRUE,
                     test.names = c("Test 1", "Test 2")) {
  
  ## CHECK ARGUMENTS
  
  if(sesp == FALSE && ppvnpv == FALSE && plrnlr == FALSE) stop("No tests selected.")
  if(!is.data.frame(df)) stop("Please provide data as a data frame with three columns.")
  if(ncol(df) != 3) stop("Please provide data as a data frame with three columns.")
  
  ## FUNCTION 
  
  vals <- values.2test(df)
  cont <- disp.cont(vals, margins = margins)
  
  glob.p.vals <- vector("numeric",0)
  p.vals <- vector("numeric", 0)

  ## Sens & Spec
  
  if(sesp == TRUE) {
    
    acc.est <- conf.acc(vals)
    acc.inf <- output.acc(vals)
    
    if(vals$n <= 100 && vals$prev <= 0.1) {
      
      if(cc == TRUE) {
        p.vect <- c(sens = acc.inf$pval3a, spec = acc.inf$pval4a)
        p.vals <- c(p.vals, p.vect)
        t.acc <- c(glob = "n < 100 and prevalence <= 10% - global test not used", 
                 sens = acc.inf$Mcc1, spec = acc.inf$Mcc2)
      } else {
        p.vect <- c(sens = acc.inf$pval3b, spec = acc.inf$pval4b)
        p.vals <- c(p.vals, p.vect)
        t.acc <- c(glob = "n < 100 and prevalence <= 10% - global test not used", 
                 sens = acc.inf$M1, spec = acc.inf$M2)
      }
      
    } else {
      glob.p.vect <- c(acc = acc.inf$glob.p)
      glob.p.vals <- c(glob.p.vals, glob.p.vect)
      t.acc <- c(glob = acc.inf$glob.t, sens = acc.inf$ind.t1, spec = acc.inf$ind.t2)
    }
    
  }
  
  ## PPV & NPV
  
  if(ppvnpv == TRUE) {
    
    pv.est <- conf.pv(vals)
    pv.inf <- output.pv(vals)
    
    glob.p.vect <- c(pv = pv.inf$glob.p)
    glob.p.vals <- c(glob.p.vals, glob.p.vect)
    t.pv <- c(glob = pv.inf$glob.t, ppv = pv.inf$ind.t1, npv = pv.inf$ind.t2)
    
  }
  
  ## PLR & NLR
  
  if(plrnlr == TRUE) {
    
    lr.est <- conf.lr(vals)
    lr.inf <- output.lr(vals)
    
    glob.p.vect <- c(lr = lr.inf$glob.p)
    glob.p.vals <- c(glob.p.vals, glob.p.vect)
    t.lr <- c(glob = lr.inf$glob.t, plr = lr.inf$ind.t1, nlr = lr.inf$ind.t2)
    
  }
  
  glob.adj <- p.adjust(glob.p.vals, method = multi_corr)
  
  for(i in 1:length(glob.adj)) {
    if(glob.adj[i] < 0.05) {
      
      name.vect <- names(glob.adj)[i]
      
      if("acc" %in% name.vect) {
        p.vect <- c(sens = acc.inf$ind.p1, spec = acc.inf$ind.p2)
        p.vals <- c(p.vals, p.vect)
      }
      
      if("pv" %in% name.vect) {
        p.vect <- c(ppv = pv.inf$ind.p1, npv = pv.inf$ind.p2)
        p.vals <- c(p.vals, p.vect)
      }
      
      if("lr" %in% name.vect) {
        p.vect <- c(plr = lr.inf$ind.p1, nlr = lr.inf$ind.p2)
        p.vals <- c(p.vals, p.vect)
      }
      
    }
      
  }
  
  p.adj <- p.adjust(p.vals, method = multi_corr)
  
  if(sesp == TRUE) {
    
    acc.mat <- matrixify(acc.est, rows = c("Sensitivity", "Specificity"), test.names = test.names)
    
    acc <- list(accuracies = acc.mat,
                glob.test.stat = unname(t.acc["glob"]),
                glob.p.value = unname(glob.p.vals["acc"]),
                glob.p.adj = unname(glob.adj["acc"]),
                sens.test.stat = unname(t.acc["sens"]),
                sens.p.value = unname(p.vals["sens"]),
                sens.p.adj = unname(p.adj["sens"]),
                spec.test.stat = unname(t.acc["spec"]),
                spec.p.value = unname(p.vals["spec"]),
                spec.p.adj = unname(p.adj["spec"])
                )
    
  }
  
  if(ppvnpv == TRUE) {
    
    pv.mat <- matrixify(pv.est, rows = c("PPV", "NPV"), test.names = test.names)
   
    pv <- list(predictive.values = pv.mat,
                glob.test.stat = unname(t.pv["glob"]),
                glob.p.value = unname(glob.p.vals["pv"]),
                glob.p.adj = unname(glob.adj["pv"]),
                ppv.test.stat = unname(t.pv["ppv"]),
                ppv.p.value = unname(p.vals["ppv"]),
                ppv.p.adj = unname(p.adj["ppv"]),
                npv.test.stat = unname(t.pv["npv"]),
                npv.p.value = unname(p.vals["npv"]),
                npv.p.adj = unname(p.adj["npv"])
                )
     
  }
  
  if(plrnlr == TRUE) {
    
    lr.mat <- matrixify(lr.est, rows = c("PLR", "NLR"), test.names = test.names)
    
    lr <- list(likelihood.ratios = lr.mat,
                glob.test.stat = unname(t.lr["glob"]),
                glob.p.value = unname(glob.p.vals["lr"]),
                glob.p.adj = unname(glob.adj["lr"]),
                plr.test.stat = unname(t.lr["plr"]),
                plr.p.value = unname(p.vals["plr"]),
                plr.p.adj = unname(p.adj["plr"]),
                nlr.test.stat = unname(t.lr["nlr"]),
                nlr.p.value = unname(p.vals["nlr"]),
                nlr.p.adj = unname(p.adj["nlr"])
                )
    
  }
  
  if(sesp == T && ppvnpv == T && plrnlr == T) {
    out <- list(cont, acc, pv, lr); names(out) <- c("cont", "acc", "pv", "lr")}
  else if(sesp == T && ppvnpv == T && plrnlr == F) {
    out <- list(cont, acc, pv); names(out) <- c("cont", "acc", "pv")}
  else if(sesp == T && ppvnpv == F && plrnlr == T) {
    out <- list(cont, acc, lr); names(out) <- c("cont", "acc", "lr")}
  else if(sesp == F && ppvnpv == T && plrnlr == T) {
    out <- list(cont, pv, lr); names(out) <- c("cont", "pv", "lr")}
  else if(sesp == T && ppvnpv == F && plrnlr == F) {
    out <- list(cont, acc); names(out) <- c("cont", "acc")}
  else if(sesp == F && ppvnpv == T && plrnlr == F) {
    out <- list(cont, pv); names(out) <- c("cont", "pv")}
  else if(sesp == F && ppvnpv == F && plrnlr == T) {
    out <- list(cont, lr); names(out) <- c("cont", "lr")}
  
  class(out) <- "comp2bdt"
  
  return(out)
}
```

# Motivating example

*for now see examples and vignette, I will prepare my retinopathy data or some data from a classic study for the paper*

# Summary

Herein we have described a suite of functions which allow simple implementation of up-to-date descriptive and inferential statistics for the comparison of two binary diagnostic tests using paired data. At its simplest, the function requires only a data frame containing the raw data, but several optional arguments allow users to customise their output.

These functions have been compiled into an R package `comp2bdt`.
